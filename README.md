Perfect ğŸš€ â€” hereâ€™s the updated **README.md template** with a **badges section** at the top. These badges make your GitHub repo look polished and professional:

```markdown
# RL Assignment 1 â€“ Grid Maze as an MDP  

![Python](https://img.shields.io/badge/python-3.8%2B-blue.svg)  
![Status](https://img.shields.io/badge/status-completed-success.svg)  
![License](https://img.shields.io/badge/license-MIT-green.svg)  

**Author:** Amar Singh (2022UG000009)  
**Course Outcome (CO1):** Apply core reinforcement learning concepts to solve real-world problems.  

---

## ğŸ“Œ Project Overview
This project implements a **6Ã—6 grid maze** environment formulated as a **Markov Decision Process (MDP)**.  
Two agents were implemented and compared:  
- **Random Policy Agent** (chooses actions uniformly at random)  
- **Greedy Manhattan Policy Agent** (chooses actions that reduce Manhattan distance to the goal)  

The project simulates **20 episodes per policy**, logs trajectories, computes statistics, and provides analysis of performance.  

---

## ğŸ—ï¸ MDP Specification
- **States (S):** Each free cell in the grid, represented as `(row, col)`  
- **Actions (A):** Up, Down, Left, Right  
- **Transitions (p(sâ€²|s,a)):** Deterministic  
  - Valid move â†’ move to next cell  
  - Invalid move (off-grid or wall) â†’ stay in same cell  
  - Goal â†’ terminal absorbing state  
- **Rewards (r(s,a,sâ€²)):**  
  - â€“1 per step  
  - â€“2 if bump into wall/off-grid  
  - +10 when reaching the goal  
- **Discount Factor (Î³):** 0.9  
- **Start:** `(5,0)` (bottom-left)  
- **Goal:** `(0,5)` (top-right)  
- **Walls:** (1,1), (1,2), (1,4), (2,4), (3,1), (3,2), (3,4), (4,4)  

---

## ğŸ“‚ Project Structure
```

RL\_Assignment1/
â”‚
â”œâ”€â”€ src/                  # Source code
â”‚   â”œâ”€â”€ maze\_env.py       # Grid environment & MDP definition
â”‚   â”œâ”€â”€ policies.py       # Random and Greedy policy implementations
â”‚   â”œâ”€â”€ simulate.py       # Episode simulation functions
â”‚   â””â”€â”€ run\_all.py        # Main script to run experiments
â”‚
â”œâ”€â”€ results/              # Output of simulations
â”‚   â”œâ”€â”€ stats\_random.csv          # Per-episode stats for Random policy
â”‚   â”œâ”€â”€ stats\_greedy.csv          # Per-episode stats for Greedy policy
â”‚   â”œâ”€â”€ summary\_random.json       # Min/Max/Avg stats (Random)
â”‚   â”œâ”€â”€ summary\_greedy.json       # Min/Max/Avg stats (Greedy)
â”‚   â”œâ”€â”€ sample\_trajectory\_random.txt
â”‚   â”œâ”€â”€ sample\_trajectory\_greedy.txt
â”‚   â”œâ”€â”€ steps\_random.png          # Plot: steps per episode (Random)
â”‚   â”œâ”€â”€ steps\_greedy.png          # Plot: steps per episode (Greedy)
â”‚   â”œâ”€â”€ rewards\_random.png        # Plot: rewards per episode (Random)
â”‚   â”œâ”€â”€ rewards\_greedy.png        # Plot: rewards per episode (Greedy)
â”‚   â””â”€â”€ comparison\_charts.pdf     # Side-by-side plots (success rate, avg steps, avg rewards)
â”‚
â”œâ”€â”€ report/
â”‚   â”œâ”€â”€ Assignment1\_Final\_Report.pdf
â”‚   â””â”€â”€ Assignment1\_Report\_Compact\_3pg.pdf
â”‚
â””â”€â”€ README.md             # This file

````

---

## â–¶ï¸ How to Run

1. **Clone the repo**
   ```bash
   git clone https://github.com/AmarSingh-11/RL_Assignment1.git
   cd RL_Assignment1
````

2. **Run all experiments**

   ```bash
   python3 src/run_all.py
   ```

3. **Outputs will be saved under `results/`**

   * Per-episode stats in CSV
   * Summary stats in JSON
   * Plots in PNG
   * Sample trajectories in TXT

---

## ğŸ“Š Results Summary

### Random Policy (20 episodes)

* Success rate: **60% (12/20)**
* Steps: min = **34**, max = **200**, avg = **141.5**
* Rewards: min = **â€“287**, max = **â€“37**, avg = **â€“188.2**

### Greedy Policy (20 episodes)

* Success rate: **100% (20/20)**
* Steps: always **10**
* Rewards: always **+1**

---

## ğŸ” Analysis & Exploration

* **Wall placement**: Forces detours; increases steps and lowers reward if direct greedy path is blocked.
* **Reward values**: Step penalty (â€“1) encourages efficiency, bump penalty (â€“2) discourages hitting walls, +10 goal reward ensures motivation.
* **Random policy**: Explores widely, sometimes finds paths, but inefficient (low reward, long steps).
* **Greedy policy**: Fast and reliable in this maze (always succeeds in 10 steps), but short-sighted in more complex layouts.

---

## ğŸš€ Suggested Improvements

* **Îµ-Greedy policy**: Mostly greedy, but sometimes random to escape traps.
* **Reward shaping**: Add small positive reward for reducing distance to goal.
* **Loop penalty**: Penalize revisiting states to avoid cycles.
* **Lookahead planning**: Consider next two steps to avoid dead ends.

---

## ğŸ“‘ Deliverables

* âœ… Well-commented source code (`src/`)
* âœ… Simulation results (`results/`)
* âœ… Final compact report (`report/Assignment1_Final_Report.pdf`)

